{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Data Loading\n",
    "\n",
    "## Imports\n",
    "\n",
    "First, we import pytorch. `nn` is used for making a Module object, which can hold models and submodules. `torchvision` contains a bunch of computer vision datasets (e.g., MNIST) and we use ToTensor to turn the data into a tensor with values scaled to between 0.0 and 1.0 unless the input datatype is not listed in the method's docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Datasets\n",
    "\n",
    "Next, we load the fashion MNIST dataset, a set of 28 x 28 px grayscale images, each associated with a label from one of 10 classes.\n",
    "\n",
    "`root`: target destination folder name\n",
    "`train`: boolean indicating whether the set is for training or not\n",
    "`download`: boolean indicating whether the set should be downloaded from the internet if not found in `root`\n",
    "`transform`: tranformation to be applied to the dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader Iterable Class\n",
    "Wrapping the datasets in a DataLoader iterable enables automatic batching, shuffling, and multiprocess data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape) # N = batch size, C = number of channels, H = height, W = width\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Building a Model\n",
    "The `NeuralNetwork` class inherits from the `nn.Module` class. The layers are defined in the `__init__` function and the `forward` function is how we specify how to pass data through the network. By moving to an accelerator, which can leverage the asynchronous capabilities of devices like GPUs, operations can be accelerated; if no accelerator is available, the CPU is used.\n",
    "\n",
    "Accelerators available include CUDA, MPS, MTIA, or XPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training; an accelerator is a device that can execute code, such as a GPU\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # Turns the 28x28 image into a 1x784 tensor by taking each row of pixels and lining them up end to end\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),  # 28*28 = 784; 512 is the number of neurons in the hidden layer; Linear applies an affine transformation to the input data\n",
    "            nn.ReLU(),  # ReLU activation function, which is max(0, x)\n",
    "            nn.Linear(512, 512),  # Another hidden layer; 512 is the number of neurons in the hidden layer\n",
    "            nn.ReLU(),  # Another ReLU activation function\n",
    "            nn.Linear(512, 10),  # Output layer; 10 is the number of classes\n",
    "        )\n",
    "\n",
    "    # The forward method defines the computation performed at every call\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the Model Parameters\n",
    "To train a model, we need:\n",
    "- A loss function\n",
    "- An optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # This loss function is used for classification problems\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)  # Stochastic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "In the training function, first the model is set to train mode via `model.train()` which affect some layers (e.g., Dropout). Then, in each iteration of the training loop, the model performs an inference on the input `X` and finds the error between that and the actual value `y`. This loss is then backward propagated, the optimizer advances the network to the next iteration and clears/resets the gradients of all model parameters before the next backprop step, which are instead accumulated in the `.backward()` step here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)  # Move the data to the device that is being used by the model; else an error will be thrown\n",
    "        \n",
    "        # Compute the prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303646 [   64/60000]\n",
      "loss: 2.297573 [ 6464/60000]\n",
      "loss: 2.282568 [12864/60000]\n",
      "loss: 2.271780 [19264/60000]\n",
      "loss: 2.257754 [25664/60000]\n",
      "loss: 2.231398 [32064/60000]\n",
      "loss: 2.237425 [38464/60000]\n",
      "loss: 2.205606 [44864/60000]\n",
      "loss: 2.200603 [51264/60000]\n",
      "loss: 2.181205 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.0%, Avg loss: 2.172919 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.185383 [   64/60000]\n",
      "loss: 2.177773 [ 6464/60000]\n",
      "loss: 2.130175 [12864/60000]\n",
      "loss: 2.135863 [19264/60000]\n",
      "loss: 2.094116 [25664/60000]\n",
      "loss: 2.039457 [32064/60000]\n",
      "loss: 2.062903 [38464/60000]\n",
      "loss: 1.992119 [44864/60000]\n",
      "loss: 1.991786 [51264/60000]\n",
      "loss: 1.931741 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 1.926050 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.962821 [   64/60000]\n",
      "loss: 1.933204 [ 6464/60000]\n",
      "loss: 1.826285 [12864/60000]\n",
      "loss: 1.854220 [19264/60000]\n",
      "loss: 1.748441 [25664/60000]\n",
      "loss: 1.703393 [32064/60000]\n",
      "loss: 1.725344 [38464/60000]\n",
      "loss: 1.624724 [44864/60000]\n",
      "loss: 1.647924 [51264/60000]\n",
      "loss: 1.546090 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: 1.557224 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.630123 [   64/60000]\n",
      "loss: 1.587027 [ 6464/60000]\n",
      "loss: 1.444802 [12864/60000]\n",
      "loss: 1.503959 [19264/60000]\n",
      "loss: 1.382574 [25664/60000]\n",
      "loss: 1.382827 [32064/60000]\n",
      "loss: 1.394308 [38464/60000]\n",
      "loss: 1.315308 [44864/60000]\n",
      "loss: 1.349608 [51264/60000]\n",
      "loss: 1.245641 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 1.274646 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.360261 [   64/60000]\n",
      "loss: 1.331281 [ 6464/60000]\n",
      "loss: 1.175124 [12864/60000]\n",
      "loss: 1.267564 [19264/60000]\n",
      "loss: 1.145767 [25664/60000]\n",
      "loss: 1.172416 [32064/60000]\n",
      "loss: 1.188179 [38464/60000]\n",
      "loss: 1.122917 [44864/60000]\n",
      "loss: 1.160546 [51264/60000]\n",
      "loss: 1.071537 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 1.099935 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "## Printing Results for Each Epoch\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
